{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "!pip install py4j boto3 awswrangler\n",
    "!pip install pyngrok\n",
    "!pip install streamlit\n",
    "import os\n",
    "\n",
    "from pyngrok import ngrok\n",
    "import sys\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from typing import List\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, mean, lit, when,month,sum\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler,PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "\n",
    "spark= SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"Weather_anomaly_detection\") \\\n",
    "       .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading and processing historical meteorological data\n",
    "\n",
    "Data reading will be done from the S3 bucket where all the CSV files containing the historical data for each city are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the list of cities\n",
    "CITIES=['Bucharest','London','Paris','Roma']\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a dropdown mechanism for choosing a single city from the list\n",
    "city_dropdown = widgets.Dropdown(\n",
    "    options=CITIES,\n",
    "    value=CITIES[0],\n",
    "    description='Please select one city:',\n",
    ")\n",
    "selected_city = city_dropdown.value     \n",
    "\n",
    "# Function to update the global variable when the value in the dropdown changes\n",
    "def on_city_change(change):\n",
    "    global selected_city\n",
    "    selected_city = change['new']\n",
    "    print(f\"Selected city is: {selected_city}\")\n",
    "\n",
    "city_dropdown.observe(on_city_change, names='value')    \n",
    "display(city_dropdown)    \n",
    "\n",
    "def get_selected_city():   \n",
    "    return selected_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_lat_lon(city_name: str):\n",
    "    url = f\"http://api.openweathermap.org/geo/1.0/direct?q={city_name.title()}&limit=5&appid=e7861e61e40771ccad4480ae27791bab\"\n",
    "\n",
    "    response = requests.request(\"GET\", url)\n",
    "\n",
    "    data = response.json()\n",
    "    return {\"lat\":data[0][\"lat\"],\"lon\":data[0][\"lon\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"latitude_longitude_{selected_city}.json\",\"w\") as f:\n",
    "  json.dump(get_lat_lon(selected_city),f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Access the archive weather data file for the selected city and download it locally\n",
    "\n",
    "AWS_ACCESS_KEY=\"\"\n",
    "AWS_SECRET_KEY=\"\"\n",
    "S3_BUCKET = \"\"\n",
    "AWS_REGION = \"\"\n",
    "\n",
    "fp=f\"s3://{S3_BUCKET}/{selected_city}_10_years.csv\"\n",
    "aws_session=boto3.Session(aws_access_key_id=AWS_ACCESS_KEY,aws_secret_access_key=AWS_SECRET_KEY,region_name=AWS_REGION)\n",
    "wr.s3.download(fp,f\"historical_data_read_{selected_city}.csv\",boto3_session=aws_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Spark transform to replace periods (.) in column names with underscores (_)\n",
    "\n",
    "df = spark.read.csv(f\"historical_data_read_{selected_city}.csv\",inferSchema=True,header=True)\n",
    "for col_name in df.columns:\n",
    "    new_col_name = col_name.replace('.', '_')\n",
    "    df = df.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "df = df.drop('lat', 'lon', 'tz')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Identifying columns with missing values\n",
    "\n",
    "null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "\n",
    "print(\"Number of null values:\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a new column 'season', indicating the season associated with the date in the date column\n",
    "\n",
    "spring_condition = (month(df['date']).between(3, 5))\n",
    "summer_condition = (month(df['date']).between(6, 8))\n",
    "autumn_condition = (month(df['date']).between(9, 11))\n",
    "winter_condition = (month(df['date']).isin([12, 1, 2]))\n",
    "\n",
    "df = df.withColumn('season',\n",
    "                   when(spring_condition, 'Spring')\n",
    "                   .when(summer_condition, 'Summer')\n",
    "                   .when(autumn_condition, 'Autumn')\n",
    "                   .when(winter_condition, 'Winter'))\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Temperature conversion from Kelvin to Celsius\n",
    "\n",
    "temperature_columns = [\n",
    "    'temperature_min', 'temperature_max', 'temperature_afternoon',\n",
    "    'temperature_night', 'temperature_evening', 'temperature_morning'\n",
    "]\n",
    "\n",
    "for col_name in temperature_columns:\n",
    "    df = df.withColumn(col_name, col(col_name) - 273.15)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Defining a function to calculate the average temperature for each season\n",
    "\n",
    "def calculate_average_temperature(season):\n",
    "    season_data = df.filter(col('season') == season)\n",
    "    avg_temp = season_data.select(mean(col('temperature_afternoon')).alias('avg_temp')).collect()[0]['avg_temp']\n",
    "    return avg_temp\n",
    "\n",
    "# average temp for each season\n",
    "spring_avg_temp = calculate_average_temperature('Spring')\n",
    "summer_avg_temp = calculate_average_temperature('Summer')\n",
    "autumn_avg_temp = calculate_average_temperature('Autumn')\n",
    "winter_avg_temp = calculate_average_temperature('Winter')\n",
    "df = df.withColumn(\n",
    "    \"season_avg_temp\",\n",
    "    when(col('season') == 'Spring', lit(spring_avg_temp))\n",
    "    .when(col('season') == 'Summer', lit(summer_avg_temp))\n",
    "    .when(col('season') == 'Autumn', lit(autumn_avg_temp))\n",
    "    .when(col('season') == 'Winter', lit(winter_avg_temp))\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating a static anomaly detection model using the threshold method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define a dictionary of thresholds for each type of anomaly\n",
    "threshold = {\n",
    "    'temperature_frost': 0,                  # Celsius\n",
    "    'temperature_heatwave': 35,              # Celsius\n",
    "    'precipitation_floods': 40,              # Millimeters/day\n",
    "    'wind_storm': 20,                        # Meters/s\n",
    "    'humidity_increased': 95,                # Percentage\n",
    "    'atmospheric_pressure': (900, 1100)      # Hectopascali\n",
    "}\n",
    "\n",
    "# Define anomalies together with their conditions\n",
    "anomaly_types = {\n",
    "    'anomaly_frost': (F.col('temperature_min') < threshold['temperature_frost']),\n",
    "    'anomaly_heatwave': (F.col('temperature_max') > threshold['temperature_heatwave']),\n",
    "    'anomaly_floods': (F.col('precipitation_total') > threshold['precipitation_floods']) ,\n",
    "    'anomaly_storm': (F.col('wind_max_speed') > threshold['wind_storm']),\n",
    "    'anomaly_humidity_increased': (F.col('humidity_afternoon') > threshold['humidity_increased']),\n",
    "    'anomaly_atmospheric_pressure': (F.col('pressure_afternoon') < threshold['atmospheric_pressure'][0]) |\n",
    "                                     (F.col('pressure_afternoon') > threshold['atmospheric_pressure'][1])\n",
    "}\n",
    "\n",
    "# Column initialization for presence and type of anomalies\n",
    "df = df.withColumn('presence_anomaly', F.lit(0).cast('int'))\n",
    "df = df.withColumn('type_anomaly', F.lit(None).cast('string'))\n",
    "\n",
    "# Update values for columns indicating the presence and type of anomalies in the DataFrame\n",
    "for anomaly, condition in anomaly_types.items():\n",
    "    df = df.withColumn('type_anomaly', F.when(condition, anomaly).otherwise(F.col('type_anomaly')))\n",
    "    df = df.withColumn('presence_anomaly', F.when(condition, 1).otherwise(F.col('presence_anomaly')))\n",
    "\n",
    "# Creating a dictionary of correspondences between anomaly type and numeric identifier\n",
    "mapping_anomalies = {anomaly: idx + 1 for idx, anomaly in enumerate(anomaly_types.keys())}\n",
    "\n",
    "df = df.na.fill('does_not_exist')\n",
    "mapping_anomalies['does_not_exist'] = 0\n",
    "\n",
    "# Convert the descriptive text of the anomaly to the corresponding numeric identifier within the column type_anomaly\n",
    "for type_anomaly, mapped_value in mapping_anomalies.items():\n",
    "    df = df.withColumn('type_anomaly', F.when(df['type_anomaly'] == type_anomaly, mapped_value).otherwise(df['type_anomaly']))\n",
    "\n",
    "df = df.withColumn('type_anomaly', df['type_anomaly'].cast('int'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Count and display the frequency of each type of anomaly detected\n",
    "\n",
    "label_counts = df.groupBy('type_anomaly').count()\n",
    "label_counts_renamed = label_counts.withColumnRenamed('count', 'frequency_appearance')\n",
    "\n",
    "label_counts_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the anomaly mapping dictionary to a JSON file\n",
    "\n",
    "import json\n",
    "with open('mapping_anomalies.json',\"w\") as f:\n",
    "  json.dump({float(v):k for k,v in mapping_anomalies.items()},f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Logic Regression machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA, StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Defining columns of meteorological characteristics\n",
    "X_cols = ['temperature_afternoon', 'temperature_min', 'temperature_max',\n",
    "          'precipitation_total', 'wind_max_direction',\n",
    "          'wind_max_speed', 'pressure_afternoon',\n",
    "          'season_avg_temp']\n",
    "\n",
    "# Combining feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=X_cols, outputCol=\"features\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# Applying Principal Component Analysis (PCA)\n",
    "num_principal_components = 4  \n",
    "pca = PCA(k=num_principal_components, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Defining the logical regression model for classifying the presence of anomalies (binary problem)\n",
    "lr_anomaly_presence = LogisticRegression(labelCol=\"presence_anomaly\", featuresCol=\"pca_features\")\n",
    "\n",
    "# Defining the logical regression model for anomaly type classification (multiclass problem)\n",
    "lr_anomaly_type = LogisticRegression(labelCol=\"type_anomaly\", featuresCol=\"pca_features\", maxIter=10, family=\"multinomial\")\n",
    "\n",
    "# Defining pipelines\n",
    "pipeline_anomaly_presence_lr = Pipeline(stages=[assembler, scaler, pca, lr_anomaly_presence])\n",
    "pipeline_anomaly_type_lr = Pipeline(stages=[assembler, scaler, pca, lr_anomaly_type])\n",
    "\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Training on training data of the 2 models for anomaly presence and type\n",
    "model_anomaly_presence_lr = pipeline_anomaly_presence_lr.fit(train_data)\n",
    "model_anomaly_type_lr = pipeline_anomaly_type_lr.fit(train_data)\n",
    "\n",
    "# Apply previously trained models on the test set to make predictions\n",
    "predictions_anomaly_presence = model_anomaly_presence_lr.transform(test_data)\n",
    "predictions_anomaly_type = model_anomaly_type_lr.transform(test_data)\n",
    "\n",
    "# Performance evaluation of the 2 logical regression models\n",
    "evaluator_anomaly_presence = BinaryClassificationEvaluator(labelCol=\"presence_anomaly\", rawPredictionCol=\"rawPrediction\")\n",
    "accuracy_anomaly_presence = evaluator_anomaly_presence.evaluate(predictions_anomaly_presence)\n",
    "print(\"Accuracy of the anomaly presence prediction model = \", accuracy_anomaly_presence)\n",
    "\n",
    "evaluator_anomaly_type = MulticlassClassificationEvaluator(labelCol=\"type_anomaly\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_anomaly_type = evaluator_anomaly_type.evaluate(predictions_anomaly_type)\n",
    "print(\"Accuracy of the anomaly type prediction model = \", accuracy_anomaly_type)\n",
    "\n",
    "# Combining predictions and values from the static threshold method in a single DataFrame for easier interpretation\n",
    "predictions_combined = predictions_anomaly_presence.select(\n",
    "    col(\"features\"),\n",
    "    col(\"scaled_features\"),\n",
    "    col(\"pca_features\"),\n",
    "    col(\"presence_anomaly\").alias(\"presence_anomaly_static\"),    \n",
    "    col(\"prediction\").alias(\"presence_anomaly_predicted\")          \n",
    ").join(\n",
    "    predictions_anomaly_type.select(\n",
    "        col(\"features\"),\n",
    "        col(\"scaled_features\"),\n",
    "        col(\"pca_features\"),\n",
    "        col(\"type_anomaly\").alias(\"type_anomaly_static\"),     \n",
    "        col(\"type_anomaly\"),\n",
    "        col(\"prediction\").alias(\"type_anomaly_predicted\")   \n",
    "    ),\n",
    "    on=[\"features\", \"scaled_features\", \"pca_features\"]\n",
    ")\n",
    "\n",
    "predictions_combined.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Random Forest machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA, StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Defining columns of meteorological characteristics\n",
    "X_cols = ['temperature_afternoon', 'temperature_min', 'temperature_max',\n",
    "          'precipitation_total', 'wind_max_direction',\n",
    "          'wind_max_speed', 'pressure_afternoon',\n",
    "          'season_avg_temp']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=X_cols, outputCol=\"features\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "num_principal_components = 4  \n",
    "pca = PCA(k=num_principal_components, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Definition of the Random Forest model to classify the presence of anomalies (binary problem)\n",
    "rf_anomaly_presence = RandomForestClassifier(labelCol=\"presence_anomaly\", featuresCol=\"pca_features\")\n",
    "\n",
    "# Definition of the Random Forest model for anomaly type classification (multiclass problem)\n",
    "rf_anomaly_type = RandomForestClassifier(labelCol=\"type_anomaly\", featuresCol=\"pca_features\")\n",
    "\n",
    "pipeline_anomaly_presence_rf = Pipeline(stages=[assembler, scaler, pca, rf_anomaly_presence])\n",
    "pipeline_anomaly_type_rf = Pipeline(stages=[assembler, scaler, pca, rf_anomaly_type])\n",
    "\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model_anomaly_presence_rf = pipeline_anomaly_presence_rf.fit(train_data)\n",
    "model_anomaly_type_rf = pipeline_anomaly_type_rf.fit(train_data)\n",
    "\n",
    "predictions_anomaly_presence = model_anomaly_presence_rf.transform(test_data)\n",
    "predictions_anomaly_type = model_anomaly_type_rf.transform(test_data)\n",
    "\n",
    "evaluator_anomaly_presence = BinaryClassificationEvaluator(labelCol=\"presence_anomaly\", rawPredictionCol=\"rawPrediction\")\n",
    "accuracy_anomaly_presence = evaluator_anomaly_presence.evaluate(predictions_anomaly_presence)\n",
    "print(\"Accuracy of the anomaly presence prediction model = \", accuracy_anomaly_presence)\n",
    "\n",
    "evaluator_anomaly_type = MulticlassClassificationEvaluator(labelCol=\"type_anomaly\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_anomaly_type = evaluator_anomaly_type.evaluate(predictions_anomaly_type)\n",
    "print(\"Accuracy of the anomaly type prediction model = \", accuracy_anomaly_type)\n",
    "\n",
    "predictions_combined = predictions_anomaly_presence.select(\n",
    "    col(\"features\"),\n",
    "    col(\"scaled_features\"),\n",
    "    col(\"pca_features\"),\n",
    "    col(\"presence_anomaly\").alias(\"presence_anomaly_static\"),    \n",
    "    col(\"prediction\").alias(\"presence_anomaly_predicted\")           \n",
    ").join(\n",
    "    predictions_anomaly_type.select(\n",
    "        col(\"features\"),\n",
    "        col(\"scaled_features\"),\n",
    "        col(\"pca_features\"),\n",
    "        col(\"type_anomaly\").alias(\"type_anomaly_static\"),     \n",
    "        col(\"type_anomaly\"),\n",
    "        col(\"prediction\").alias(\"type_anomaly_predicted\")      \n",
    "    ),\n",
    "    on=[\"features\", \"scaled_features\", \"pca_features\"]\n",
    ")\n",
    "\n",
    "predictions_combined.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Saving trained models with better performance (logic regression for example)\n",
    "\n",
    "model_path=\"model_presence_anomaly\"\n",
    "model_anomaly_presence_lr.write().overwrite().save(model_path)\n",
    "\n",
    "model_path=\"model_type_anomaly\"\n",
    "model_anomaly_type_lr.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Python app implementation using Streamlit interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import json\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, mean, lit, when,month\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql import Row\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from typing import List\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark= SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"Spark Session\") \\\n",
    "       .getOrCreate()\n",
    "\n",
    "\n",
    "with open('mapping_anomalies.json','r') as f:\n",
    "  mapping_anomalies_data=json.load(f)\n",
    "  mapping_anomalies_data={float(k):v for k,v in mapping_anomalies_data.items()}\n",
    "\n",
    "import smtplib\n",
    "import requests\n",
    "import datetime\n",
    "import re\n",
    "import smtplib\n",
    "import ssl\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "import os\n",
    "\n",
    "password = \"\"\n",
    "sender_email=\"georgiana.iordache300@gmail.com\"\n",
    "city_name=\"Roma\"\n",
    "\n",
    "def generate_html_content():\n",
    "    # Start of the HTML content\n",
    "    html_content = \"\"\"\n",
    "    <html>\n",
    "    <body>\n",
    "        <h2>Result of the weather anomalies analysis</h2>\n",
    "        <p>Download the attached file to see the forecast for the next few days.</p>\n",
    "        <p>Thank you for choosing to use our app!</p>\n",
    "  </body>\n",
    "    </html>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return html_content\n",
    "\n",
    "\n",
    "# Define the function to send mails to application users\n",
    "def send_email(sender_email,recipient_email,password,content,subject=f\"Weather anomaly detection for {city_name}\",prediction_path=f\"analysis_results_{city_name}.csv\"):\n",
    "\n",
    "    sender_email = sender_email\n",
    "    recipient_email = recipient_email\n",
    "    subject = subject\n",
    "\n",
    "    msg = MIMEMultipart()\n",
    "\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = recipient_email\n",
    "    msg['Subject'] = subject\n",
    "\n",
    "    smtp_server = \"smtp.gmail.com\"   \n",
    "    port=465   \n",
    "\n",
    "    part1 = MIMEText(content, 'html')\n",
    "    msg.attach(part1)\n",
    "\n",
    "    attachment = open(prediction_path, \"rb\")  \n",
    "    part2 = MIMEBase(\"application\", \"octet-stream\")   \n",
    "    part2.set_payload((attachment).read())\n",
    "    encoders.encode_base64(part2)  \n",
    "    part2.add_header(\n",
    "        \"Content-Disposition\",\n",
    "        f\"attachment; filename= {os.path.basename(prediction_path)}\",\n",
    "    )\n",
    "\n",
    "    msg.attach(part2)\n",
    "\n",
    "    try:\n",
    "        context = ssl.create_default_context()  \n",
    "        with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:   \n",
    "            server.login(sender_email, password)    \n",
    "            server.sendmail(sender_email, recipient_email, msg.as_string())\n",
    "    except Exception as e:\n",
    "        print(f\"Error in submission process: {e}\")\n",
    "\n",
    "def read_lat_lon(city_name):\n",
    "  with open(f\"latitude_longitude_{city_name}.json\",\"r\") as f:\n",
    "    data=json.load(f)\n",
    "  return data\n",
    "\n",
    "data = read_lat_lon(city_name)\n",
    "api_key = \"\"\n",
    "\n",
    "def get_weather_data(date):\n",
    "    lat = data[\"lat\"]\n",
    "    lon = data[\"lon\"]\n",
    "    openweathermap_base_url = f\"https://api.openweathermap.org/data/3.0/onecall/day_summary?lat={lat}&lon={lon}&units=metric&date={date}&appid={api_key}\"\n",
    "    response = requests.get(openweathermap_base_url)\n",
    "    print(response.content)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_date_range(day):\n",
    "    today = datetime.datetime.now()\n",
    "    date_range = [\n",
    "        (today + datetime.timedelta(days=i))\n",
    "        .replace(hour=12, minute=0, second=0)\n",
    "        .strftime(\"%Y-%m-%d\")\n",
    "        for i in range(day)\n",
    "    ]\n",
    "    return date_range\n",
    "\n",
    "def calculate_avg_temp(df, season):\n",
    "    season_data = df[df['season'] == season]\n",
    "    avg_temp = season_data['temperature_afternoon'].mean()\n",
    "    return avg_temp\n",
    "\n",
    "def preprocess_data(df):\n",
    "\n",
    "    df.columns = [col.replace('.', '_') for col in df.columns]\n",
    "    df = df.drop(columns=['tz', 'lat', 'lon'])\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])  \n",
    "    df['month'] = df['date'].dt.month  \n",
    "\n",
    "    conditions = [\n",
    "        df['month'].between(3, 5),\n",
    "        df['month'].between(6, 8),\n",
    "        df['month'].between(9, 11),\n",
    "        df['month'].isin([12, 1, 2])\n",
    "    ]\n",
    "    choices = ['Spring', 'Summer', 'Autumn', 'Winter']\n",
    "    df['season'] = np.select(conditions, choices, default=None)\n",
    "\n",
    "    spring_avg_temp = calculate_avg_temp(df, 'Spring')\n",
    "    summer_avg_temp      = calculate_avg_temp(df, 'Summer')\n",
    "    autumn_avg_temp    = calculate_avg_temp(df, 'Autumn')\n",
    "    winter_avg_temp     = calculate_avg_temp(df, 'Winter')\n",
    "\n",
    "    avg_temp_map = {\n",
    "        'Spring': spring_avg_temp,\n",
    "        'Summer': summer_avg_temp,\n",
    "        'Autumn': autumn_avg_temp,\n",
    "        'Winter': winter_avg_temp\n",
    "    }\n",
    "    df['season_avg_temp'] = df['season'].map(avg_temp_map)\n",
    "\n",
    "    X_cols = [\n",
    "        'temperature_afternoon', 'temperature_min', 'temperature_max',\n",
    "        'precipitation_total', 'wind_max_direction',\n",
    "        'wind_max_speed', 'pressure_afternoon',\n",
    "        'season_avg_temp'\n",
    "    ]\n",
    "    X_future = df[X_cols]\n",
    "\n",
    "    return X_future\n",
    "\n",
    "def fetch_weather_data_with_progress(days):\n",
    "    date_range = generate_date_range(day=days)  \n",
    "    total_dates = len(date_range)\n",
    "    progress_bar = st.progress(0)   \n",
    "\n",
    "    future_data = []     \n",
    "    for idx, dat in enumerate(date_range):\n",
    "        future_data.append(get_weather_data(date=dat))  \n",
    "        progress_bar.progress((idx + 1) / total_dates)  \n",
    "\n",
    "    return future_data\n",
    "\n",
    "st.title(f\"{city_name} - Weather Alert\")\n",
    "model_anomaly_presence = PipelineModel.load(\"model_presence_anomaly\")\n",
    "model_anomaly_type = PipelineModel.load(\"model_type_anomaly\")\n",
    "\n",
    "# Streamlit form to collect user's personal data\n",
    "with st.form(key=\"form\"):\n",
    "    days = st.slider(label=\"Please introduce the number of days for which you want the weather analysis\", min_value=7, max_value=90)\n",
    "    first_name = st.text_input(label=\"First name\")\n",
    "    last_name = st.text_input(label=\"Last name\")\n",
    "    email_address = st.text_input(label=\"Email address\")\n",
    "    submit = st.form_submit_button(label=\"Submit\")\n",
    "\n",
    "    if submit:\n",
    "            future_data = fetch_weather_data_with_progress(days=days)\n",
    "            df = pd.json_normalize(future_data)\n",
    "\n",
    "            # Redenumire coloane\n",
    "            df.rename(columns={\n",
    "              'date': 'date',\n",
    "              'units': 'measurement_unit',\n",
    "              'cloud_cover.afternoon': 'cloud_cover_afternoon',\n",
    "              'humidity.afternoon': 'humidity_afternoon',\n",
    "              'precipitation.total': 'precipitation_total',\n",
    "              'temperature.min': 'temperature_min',\n",
    "              'temperature.max': 'temperature_max',\n",
    "              'temperature.afternoon': 'temperature_afternoon',\n",
    "              'temperature.night': 'temperature_night',\n",
    "              'temperature.evening': 'temperature_evening',\n",
    "              'temperature.morning': 'temperature_morning',\n",
    "              'pressure.afternoon': 'atm_pressure_afternoon',\n",
    "              'wind.max.speed': 'wind_max_speed',\n",
    "              'wind.max.direction': 'wind_max_direction'\n",
    "          }, inplace=True)\n",
    "\n",
    "            st.dataframe(df)               \n",
    "            dates_list = df['date']\n",
    "            df = preprocess_data(df)        \n",
    "\n",
    "            df.to_csv(\"future_data.csv\", index=False) \n",
    "\n",
    "            sparkdf = spark.read.csv(\"future_data.csv\", inferSchema=True, header=True)\n",
    "\n",
    "            predictions_presence = model_anomaly_presence.transform(sparkdf)\n",
    "            predictions_type = model_anomaly_type.transform(sparkdf)\n",
    "\n",
    "            predictions_pd = predictions_presence.select(\"prediction\").toPandas()\n",
    "            predictions_pd['prediction'] = predictions_pd['prediction'].map({0: \"Normal\", 1: \"Anomaly\"})\n",
    "\n",
    "            anomaly_type_predictions = predictions_type.select(\"prediction\").toPandas()\n",
    "\n",
    "            predictions_pd.columns = ['anomaly_presence']  \n",
    "            anomaly_type_predictions.columns = ['anomaly_type']  \n",
    "\n",
    "            combined_df = pd.concat([dates_list, predictions_pd['anomaly_presence'], anomaly_type_predictions['anomaly_type']], axis=1)\n",
    "            if 'anomaly_presence' in combined_df.columns:\n",
    "                combined_df = combined_df.drop(columns=['anomaly_presence'])\n",
    "\n",
    "            combined_df.columns = ['date', 'type_anomaly']\n",
    "\n",
    "            combined_df['type_anomaly']=combined_df['type_anomaly'].map(mapping_anomalies_data)\n",
    "            combined_df['type_anomaly']=combined_df['type_anomaly'].replace('does_not_exist',None)\n",
    "\n",
    "            combined_df.to_csv(f\"analysis_results_{city_name}.csv\", index=False)\n",
    "            content = generate_html_content()\n",
    "\n",
    "            send_email(sender_email, email_address, password, content)\n",
    "\n",
    "            st.success(\"The results have been sent. Please check your e-mail address.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ngrok config add-authtoken 2hmGuq3qcffMbDDHnfPWKyB2XFe_AeBmF2fegTgKwk2iJDmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "port = \"8501\"\n",
    "public_url = ngrok.connect(port).public_url\n",
    "print(public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

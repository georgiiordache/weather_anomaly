{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "!pip install py4j boto3 awswrangler\n",
    "!pip install pyngrok\n",
    "!pip install streamlit\n",
    "import os\n",
    "\n",
    "from pyngrok import ngrok\n",
    "import sys\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from typing import List\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, mean, lit, when,month,sum\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler,PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "\n",
    "spark= SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"Detectare_anomalie\") \\\n",
    "       .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading and processing historical meteorological data\n",
    "\n",
    "Data reading will be done from the S3 bucket where all the CSV files containing the historical data for each city are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the list of cities\n",
    "ORASE=['Bucuresti','Londra','Paris','Roma']\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a dropdown mechanism for choosing a single city from the list\n",
    "oras_dropdown = widgets.Dropdown(\n",
    "    options=ORASE,\n",
    "    value=ORASE[0],\n",
    "    description='Alegere oras:',\n",
    ")\n",
    "oras_selectat = oras_dropdown.value     \n",
    "\n",
    "# Function to update the global variable when the value in the dropdown changes\n",
    "def on_city_change(change):\n",
    "    global oras_selectat\n",
    "    oras_selectat = change['new']\n",
    "    print(f\"Orasul selectat este: {oras_selectat}\")\n",
    "\n",
    "oras_dropdown.observe(on_city_change, names='value')    \n",
    "display(oras_dropdown)    \n",
    "\n",
    "def get_oras_selectat():   \n",
    "    return oras_selectat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_lat_lon(city_name: str):\n",
    "    url = f\"http://api.openweathermap.org/geo/1.0/direct?q={city_name.title()}&limit=5&appid=e7861e61e40771ccad4480ae27791bab\"\n",
    "\n",
    "    response = requests.request(\"GET\", url)\n",
    "\n",
    "    data = response.json()\n",
    "    return {\"lat\":data[0][\"lat\"],\"lon\":data[0][\"lon\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"latitudine_longitudine_{oras_selectat}.json\",\"w\") as f:\n",
    "  json.dump(get_lat_lon(oras_selectat),f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Access the archive weather data file for the selected city and download it locally\n",
    "\n",
    "AWS_ACCESS_KEY=\"AKIA5KLOABSNUMTVKZEJ\"\n",
    "AWS_SECRET_KEY=\"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\"\n",
    "S3_BUCKET = \"date-meteo-istorice\"\n",
    "AWS_REGION = \"eu-west-1\"\n",
    "\n",
    "fp=f\"s3://{S3_BUCKET}/{oras_selectat}_10_ani.csv\"\n",
    "aws_session=boto3.Session(aws_access_key_id=AWS_ACCESS_KEY,aws_secret_access_key=AWS_SECRET_KEY,region_name=AWS_REGION)\n",
    "wr.s3.download(fp,f\"date_istorice_citite_{oras_selectat}.csv\",boto3_session=aws_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Spark transform to replace periods (.) in column names with underscores (_)\n",
    "\n",
    "df = spark.read.csv(f\"date_istorice_citite_{oras_selectat}.csv\",inferSchema=True,header=True)\n",
    "for col_name in df.columns:\n",
    "    new_col_name = col_name.replace('.', '_')\n",
    "    df = df.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "# Stergerea coloanelor lat, lon, tz\n",
    "df = df.drop('lat', 'lon', 'tz')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Renaming columns \n",
    "\n",
    "df = df.withColumnRenamed('date', 'data') \\\n",
    "       .withColumnRenamed('units', 'unitate_masura') \\\n",
    "       .withColumnRenamed('cloud_cover_afternoon', 'acoperire_nori_dupaamiaza') \\\n",
    "       .withColumnRenamed('humidity_afternoon', 'umiditate_dupaamiaza') \\\n",
    "       .withColumnRenamed('precipitation_total', 'precipitatii_total') \\\n",
    "       .withColumnRenamed('temperature_min', 'temperatura_minima') \\\n",
    "       .withColumnRenamed('temperature_max', 'temperatura_maxima') \\\n",
    "       .withColumnRenamed('temperature_afternoon', 'temperatura_dupaamiaza') \\\n",
    "       .withColumnRenamed('temperature_night', 'temperatura_noaptea') \\\n",
    "       .withColumnRenamed('temperature_evening', 'temperatura_seara') \\\n",
    "       .withColumnRenamed('temperature_morning', 'temperatura_dimineata') \\\n",
    "       .withColumnRenamed('pressure_afternoon', 'presiune_atm_dupaamiaza') \\\n",
    "       .withColumnRenamed('wind_max_speed', 'viteza_max_vant') \\\n",
    "       .withColumnRenamed('wind_max_direction', 'directie_vant_max')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Identifying columns with missing values\n",
    "\n",
    "null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "\n",
    "print(\"Numărul de valori nule:\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a new column 'anotimp', indicating the season associated with the date in the date column\n",
    "\n",
    "conditie_primavara = (month(df['data']).between(3, 5))\n",
    "conditie_vara = (month(df['data']).between(6, 8))\n",
    "conditie_toamna = (month(df['data']).between(9, 11))\n",
    "conditie_iarna = (month(df['data']).isin([12, 1, 2]))\n",
    "\n",
    "df = df.withColumn('anotimp',\n",
    "                   when(conditie_primavara, 'Primavara')\n",
    "                   .when(conditie_vara, 'Vara')\n",
    "                   .when(conditie_toamna, 'Toamna')\n",
    "                   .when(conditie_iarna, 'Iarna'))\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Temperature conversion from Kelvin to Celsius\n",
    "\n",
    "coloane_temperaturi = [\n",
    "    'temperatura_minima', 'temperatura_maxima', 'temperatura_dupaamiaza',\n",
    "    'temperatura_noaptea', 'temperatura_seara', 'temperatura_dimineata'\n",
    "]\n",
    "\n",
    "for col_name in coloane_temperaturi:\n",
    "    df = df.withColumn(col_name, col(col_name) - 273.15)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Defining a function to calculate the average temperature for each season\n",
    "def calculate_average_temperature(anotimp):\n",
    "    anotimp_data = df.filter(col('anotimp') == anotimp)\n",
    "    avg_temp = anotimp_data.select(mean(col('temperatura_dupaamiaza')).alias('avg_temp')).collect()[0]['avg_temp']\n",
    "    return avg_temp\n",
    "\n",
    "# average temp for each season\n",
    "primavara_avg_temp = calculate_average_temperature('Primavara')\n",
    "vara_avg_temp = calculate_average_temperature('Vara')\n",
    "toamna_avg_temp = calculate_average_temperature('Toamna')\n",
    "iarna_avg_temp = calculate_average_temperature('Iarna')\n",
    "df = df.withColumn(\n",
    "    \"anotimp_temp_medie\",\n",
    "    when(col('anotimp') == 'Primavara', lit(primavara_avg_temp))\n",
    "    .when(col('anotimp') == 'Vara', lit(vara_avg_temp))\n",
    "    .when(col('anotimp') == 'Toamna', lit(toamna_avg_temp))\n",
    "    .when(col('anotimp') == 'Iarna', lit(iarna_avg_temp))\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating a static anomaly detection model using the threshold method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define a dictionary of thresholds for each type of anomaly\n",
    "prag = {\n",
    "    'temperatura_inghet': 0,                 # Celsius\n",
    "    'temperatura_canicula': 35,              # Celsius\n",
    "    'precipitatii_inundatii': 40,            # Millimetri/zi\n",
    "    'vant_furtuna': 20,                      # Metri/s\n",
    "    'umiditate_crescuta': 95,                # Percentage\n",
    "    'presiune_atmosferica': (900, 1100)      # Hectopascali\n",
    "}\n",
    "\n",
    "# Define anomalies together with their conditions\n",
    "tipuri_anomalie = {\n",
    "    'anomalie_inghet': (F.col('temperatura_minima') < prag['temperatura_inghet']),\n",
    "    'anomalie_canicula': (F.col('temperatura_maxima') > prag['temperatura_canicula']),\n",
    "    'anomalie_inundatii': (F.col('precipitatii_total') > prag['precipitatii_inundatii']) ,\n",
    "    'anomalie_furtuna': (F.col('viteza_max_vant') > prag['vant_furtuna']),\n",
    "    'anomalie_umiditate_crescuta': (F.col('umiditate_dupaamiaza') > prag['umiditate_crescuta']),\n",
    "    'anomalie_presiune_atmosferica': (F.col('presiune_atm_dupaamiaza') < prag['presiune_atmosferica'][0]) |\n",
    "                                     (F.col('presiune_atm_dupaamiaza') > prag['presiune_atmosferica'][1])\n",
    "}\n",
    "\n",
    "# Column initialization for presence and type of anomalies\n",
    "df = df.withColumn('prezenta_anomalie', F.lit(0).cast('int'))\n",
    "df = df.withColumn('tip_anomalie', F.lit(None).cast('string'))\n",
    "\n",
    "# Actualizarea valorilor pentru coloanele care indică prezența și tipul anomaliilor în DataFrame\n",
    "for anomalie, conditie in tipuri_anomalie.items():\n",
    "    df = df.withColumn('tip_anomalie', F.when(conditie, anomalie).otherwise(F.col('tip_anomalie')))\n",
    "    df = df.withColumn('prezenta_anomalie', F.when(conditie, 1).otherwise(F.col('prezenta_anomalie')))\n",
    "\n",
    "# Crearea unei dicționar de corespondențe între tip anomalie și identificator numeric\n",
    "mapare_anomalii = {anomalie: idx + 1 for idx, anomalie in enumerate(tipuri_anomalie.keys())}\n",
    "\n",
    "df = df.na.fill('nu_exista')\n",
    "mapare_anomalii['nu_exista'] = 0\n",
    "\n",
    "# Convertirea textului descriptiv al anomaliei în identificatorul numeric corespunzător în cadrul coloanei tip_anomalie\n",
    "for tip_anomalie, valoare_mapata in mapare_anomalii.items():\n",
    "    df = df.withColumn('tip_anomalie', F.when(df['tip_anomalie'] == tip_anomalie, valoare_mapata).otherwise(df['tip_anomalie']))\n",
    "\n",
    "df = df.withColumn('tip_anomalie', df['tip_anomalie'].cast('int'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
